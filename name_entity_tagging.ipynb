{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9a913e-cedf-49e3-9a36-0790f9e12b15",
   "metadata": {},
   "source": [
    "# Named Entity Tagging and Normalization\n",
    "- In this notebook, we will develop systems for two subtasks of the Bacteria Biotope Task (https://sites.google.com/view/bb-2019/task-description).\n",
    "    1) **BB-norm:** Normalization of Microorganism, Habitat, and Phenotype entities with NCBI Taxonomy taxa (for the former) and OntoBiotope habitat concepts (for the last two).\n",
    "    2) **BB-norm+ner:** Recognition of Microorganism, Habitat, and Phenotype entities and normalization with NCBI Taxonomy taxa and OntoBiotope habitat concepts.\n",
    "- In the BB-norm of the Bacteria Biotope task, we will develop a biomedical named entity normalizer to link the named entities (Microorganism, Habitat, and Phenotype) in a given text through a given ontology when the entities are already given their boundaries.\n",
    "- We will assume that the entities are embedded as noun phrases to achieve this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8a05c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pronto\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import spacy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc808b3-3b86-46a2-91bf-2ae47d4cee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stopwords from nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b54f0-315b-4f0b-a264-7b31735382bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_directory = r\"C:\\Users\\said_\\OneDrive\\Masaüstü\\github\\Natural Language Processing\\Datasets\\Resources\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b98965-3461-4d86-8547-45f07364cfc7",
   "metadata": {},
   "source": [
    "## 1) Search Mechanism within OntoBiotope Ontology and NCBI Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b7156-34bf-4234-95d6-36aaeec28baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained word2vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce2d65",
   "metadata": {},
   "source": [
    "### a. Check OntoBiotope Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OBO file\n",
    "obo_file_path = os.path.join(resource_directory, \"OntoBiotope_BioNLP-OST-2019.obo\")\n",
    "ontology = pronto.Ontology(obo_file_path)\n",
    "\n",
    "# Extract term IDs and names\n",
    "onto_entities = [(term.id, term.name) for term in ontology.terms()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd965745-65a3-432a-a0c9-1cfcb9423e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text by converting to lowercase and removing unnecessary characters\n",
    "def preprocess_input_text(input_text):\n",
    "    # Convert to lowercase\n",
    "    input_text = input_text.lower()\n",
    "\n",
    "    # Remove abbreviations with dots and any following word starting with the same letter\n",
    "    input_text = re.sub(r'\\b(\\w\\.)\\s*\\w*?\\b', '', input_text)\n",
    "\n",
    "    # Remove leading and trailing whitespaces\n",
    "    input_text = input_text.strip()\n",
    "\n",
    "    return input_text\n",
    "\n",
    "# Check if input text is present in ontology, then calculate cosine similarity\n",
    "def check_ontobiotope_match(input_text, onto_entities):\n",
    "    input_text = preprocess_input_text(input_text)\n",
    "    input_tokens = input_text.split()\n",
    "    terms_to_check = input_tokens + [input_text]\n",
    "    \n",
    "    matches = []\n",
    "\n",
    "    # Calculate similarity scores for each ontology entity\n",
    "    for term_id, term_name in onto_entities:\n",
    "        term_name_processed = preprocess_input_text(term_name)\n",
    "        term_tokens = term_name_processed.split()\n",
    "\n",
    "        # Check for exact matches of any term in terms_to_check\n",
    "        if any(term == term_name_processed or term in term_tokens for term in terms_to_check):\n",
    "            # Get word vectors for each token and compute the average vector\n",
    "            input_vectors = [word2vec_model[token] for token in input_tokens if token in word2vec_model]\n",
    "            term_vectors = [word2vec_model[token] for token in term_tokens if token in word2vec_model]\n",
    "\n",
    "            # Skip if no vectors found for input or term\n",
    "            if not input_vectors or not term_vectors:\n",
    "                continue \n",
    "\n",
    "            input_vector = np.mean(input_vectors, axis=0).reshape(1, -1)\n",
    "            term_vector = np.mean(term_vectors, axis=0).reshape(1, -1)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarity = cosine_similarity(input_vector, term_vector)[0][0]\n",
    "            matches.append((term_id, term_name, similarity))\n",
    "\n",
    "    if matches:\n",
    "        # Sort matches by similarity score\n",
    "        matches.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Get the best match\n",
    "        best_match_id, best_match_name, best_similarity_score = matches[0]\n",
    "        return best_match_id, best_match_name, best_similarity_score\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input text\n",
    "input_text = \"human animal plant\"\n",
    "best_match_id, best_match_name, similarity_score = check_ontobiotope_match(input_text, onto_entities)\n",
    "\n",
    "if best_match_id:\n",
    "    print(f\"Best match ID: {best_match_id}\")\n",
    "    print(f\"Best match Name: {best_match_name}\")\n",
    "    print(f\"Cosine Similarity Score: {similarity_score}\")\n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b9dc7",
   "metadata": {},
   "source": [
    "### b. Check NCBI Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ncbi_taxonomy_info(file_path):\n",
    "    entities = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('|')\n",
    "            if len(parts) >= 2:\n",
    "                entity_id = parts[0].strip()\n",
    "                entity_name = parts[1].strip()\n",
    "                entities.append((entity_id, entity_name))\n",
    "    return entities\n",
    "\n",
    "# Load the names.dmp file\n",
    "ncbi_taxonomy_file_path = os.path.join(resource_directory, \"names.dmp\")\n",
    "ncbi_taxonomy_entities = extract_ncbi_taxonomy_info(ncbi_taxonomy_file_path)\n",
    "\n",
    "# Print first 10 entities as a sample\n",
    "print(ncbi_taxonomy_entities[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e34408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text by converting to lowercase and removing unnecessary characters\n",
    "def preprocess_input_text(input_text):\n",
    "    # Convert to lowercase\n",
    "    input_text = input_text.lower()\n",
    "\n",
    "    # Remove abbreviations with dots and any following word starting with the same letter\n",
    "    input_text = re.sub(r'\\b(\\w\\.)\\s*\\w*?\\b', '', input_text)\n",
    "\n",
    "    # Remove leading and trailing whitespaces\n",
    "    input_text = input_text.strip()\n",
    "\n",
    "    return input_text\n",
    "\n",
    "# Check if input text matches with entities from NCBI Taxonomy and calculate cosine similarity\n",
    "def check_ncbi_taxonomy_match(input_text, ncbi_taxonomy_entities):\n",
    "    input_text = preprocess_input_text(input_text)\n",
    "    input_tokens = input_text.split()\n",
    "    terms_to_check = input_tokens + [input_text]\n",
    "    \n",
    "    matches = []\n",
    "\n",
    "    # Calculate similarity scores for each NCBI Taxonomy entity\n",
    "    for entity_id, entity_name in ncbi_taxonomy_entities:\n",
    "        entity_name_processed = preprocess_input_text(entity_name)\n",
    "        entity_tokens = entity_name_processed.split()\n",
    "\n",
    "        # Check for exact matches of any term in terms_to_check\n",
    "        if any(term == entity_name_processed or term in entity_tokens for term in terms_to_check):\n",
    "            # Get word vectors for each token and compute the average vector\n",
    "            input_vectors = [word2vec_model[token] for token in input_tokens if token in word2vec_model]\n",
    "            entity_vectors = [word2vec_model[token] for token in entity_tokens if token in word2vec_model]\n",
    "\n",
    "            # Skip if no vectors found for input or entity\n",
    "            if not input_vectors or not entity_vectors:\n",
    "                continue \n",
    "\n",
    "            input_vector = np.mean(input_vectors, axis=0).reshape(1, -1)\n",
    "            entity_vector = np.mean(entity_vectors, axis=0).reshape(1, -1)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarity = cosine_similarity(input_vector, entity_vector)[0][0]\n",
    "            matches.append((entity_id, entity_name, similarity))\n",
    "\n",
    "    if matches:\n",
    "        # Sort matches by similarity score\n",
    "        matches.sort(key=lambda x: x[2], reverse=True)\n",
    "        # Get the best match\n",
    "        best_match_id, best_match_name, best_similarity_score = matches[0]\n",
    "        return best_match_id, best_match_name, best_similarity_score\n",
    "    \n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"abc\"\n",
    "best_match_id, best_match_name, similarity_score = check_ncbi_taxonomy_match(input_text, ncbi_taxonomy_entities)\n",
    "\n",
    "if best_match_id:\n",
    "    print(f\"Best match ID: {best_match_id}\")\n",
    "    print(f\"Best match Name: {best_match_name}\")\n",
    "    print(f\"Cosine Similarity Score: {similarity_score}\")\n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b25ed-c100-4a3f-94fe-7a6bfc3f239b",
   "metadata": {},
   "source": [
    "### c. Put them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f5d6a-e3f2-4f8b-94ad-4f09841bf6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match(input_text, onto_entities, ncbi_taxonomy_entities):\n",
    "    onto_match_id, onto_match_name, onto_similarity = check_ontobiotope_match(input_text, onto_entities)\n",
    "    ncbi_match_id, ncbi_match_name, ncbi_similarity = check_ncbi_taxonomy_match(input_text, ncbi_taxonomy_entities)\n",
    "    \n",
    "    # Handle the case where one or both similarities are None\n",
    "    if onto_similarity is None:\n",
    "        onto_similarity = 0\n",
    "    if ncbi_similarity is None:\n",
    "        ncbi_similarity = 0\n",
    "    \n",
    "    if onto_similarity > ncbi_similarity:\n",
    "        return \"Habitat or Phenotype\", onto_match_name, onto_match_id\n",
    "    elif ncbi_similarity > onto_similarity:\n",
    "        return \"NCBI Taxonomy\", ncbi_match_name, ncbi_match_id\n",
    "    else:\n",
    "        # Handle the case where both similarities are equal\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99564f9f-05eb-49fc-80c2-b8e4ec9ccb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input text\n",
    "input_text = \"respiratory syncytial virus\"\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the function and perform similarity check\n",
    "entity_type, best_match_name, best_match_id = get_best_match(input_text, onto_entities, ncbi_taxonomy_entities)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the runtime\n",
    "runtime = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "if best_match_id:\n",
    "    print(f\"Entity Type: {entity_type}\")\n",
    "    print(f\"Best match name: {best_match_name}\")\n",
    "    print(f\"Best match ID: {best_match_id}\")\n",
    "else:\n",
    "    print(\"No match found.\")\n",
    "\n",
    "print(f\"Runtime (min): {(runtime/60):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814b343-d002-4468-b5f3-8bae0e514b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_text_files(directory):\n",
    "    txt_files = [filename for filename in os.listdir(directory) if filename.endswith(\".txt\")]\n",
    "    return len(txt_files)\n",
    "\n",
    "# Example usage\n",
    "input_file_directory = os.path.join(resource_directory, \"BioNLP-OST-2019_BB-norm+ner_test\")\n",
    "num_text_files = count_text_files(input_file_directory)\n",
    "print(f\"Number of text files in the directory: {num_text_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941ec6b-9b57-4586-b31c-3a5581bd0e41",
   "metadata": {},
   "source": [
    "**Note:** We have 96 test documents. Assuming each document contains an average of 40 entities, checking for matches would take approximately 125 hours calculated from around 80 minutes for each run. To simplify the analysis, , we will focus solely on matches within the OntoBiotope Ontology and label them as 'Habitat or Phenotype' entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ddbf8-75b1-4a69-a186-05500d532310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_best_match(input_text, onto_entities):\n",
    "    onto_match_id, onto_match_name, onto_similarity = check_ontobiotope_match(input_text, onto_entities)\n",
    "    \n",
    "    # Handle the case where similarity is None\n",
    "    if onto_similarity is None:\n",
    "        onto_similarity = 0\n",
    "    \n",
    "    return \"Habitat or Phenotype\", onto_match_name, onto_match_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input text\n",
    "input_text = \"respiratory syncytial virus\"\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the function and perform similarity check\n",
    "entity_type, best_match_name, best_match_id = adjusted_best_match(input_text, onto_entities)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the runtime\n",
    "runtime = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "if best_match_id:\n",
    "    print(f\"Entity Type: {entity_type}\")\n",
    "    print(f\"Best match name: {best_match_name}\")\n",
    "    print(f\"Best match ID: {best_match_id}\")\n",
    "else:\n",
    "    print(\"No match found.\")\n",
    "\n",
    "print(f\"Runtime (seconds): {(runtime):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074423a-e17f-42e1-9c6f-56e991de4994",
   "metadata": {},
   "source": [
    "## 2) Name Entity Recognition (NER) Model to Generate A1 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c793a-ca3b-462c-896e-960de494762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a full spaCy pipeline for biomedical data \n",
    "spacy_directory = os.path.join(resource_directory, \"en_core_sci_sm-0.5.4.tar.gz\")\n",
    "# Use double quotes around the file path to prevent parsing issues\n",
    "!pip install \"{spacy_directory}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2fac9-0d7e-4b48-8d40-40a3a763c882",
   "metadata": {},
   "source": [
    "### NER for a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc3c07-2be6-4c60-a852-79bd8e5ba8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained biomedical Named Entity Recognition (NER) model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"The etiologic and epidemiologic spectrum of bronchiolitis in pediatric practice.\n",
    "To develop a broad understanding of the causes and patterns of occurrence of wheezing associated respiratory infections, we analyzed data from an 11-year study of acute lower respiratory illness in a pediatric practice. Although half of the WARI occurred in children less than 2 years of age, wheezing continued to be observed in 19% of children greater than 9 years of age who had lower respiratory illness. Males experienced LRI 1.25 times more often than did females; the relative risk of males for WARI was 1.35. A nonbacterial pathogen was recovered from 21% of patients with WARI; respiratory syncytial virus, parainfluenza virus types 1 and 3, adenoviruses, and Mycoplasma pneumoniae accounted for 81% of the isolates. Patient age influenced the pattern of recovery of these agents. The most common cause of WARI in children under 5 years of age was RSV whereas Mycoplasma pneumoniae was the most frequent isolate from school age children with wheezing illness. The data expand our understanding of the causes of WARI and are useful to diagnosticians and to researchers interested in the control of lower respiratory disease.\"\"\"\n",
    "\n",
    "# Split the text into title and paragraph\n",
    "title_end_idx = text.find('\\n')\n",
    "title = text[:title_end_idx].strip()\n",
    "paragraph = text[title_end_idx + 1:].strip()\n",
    "\n",
    "# Output in .a1 format\n",
    "output = []\n",
    "\n",
    "# Add the title as T1\n",
    "output.append(f\"T1\\tTitle 0 {title_end_idx}\\t{title}\")\n",
    "\n",
    "# Add the paragraph as T2\n",
    "paragraph_start_idx = title_end_idx + 1\n",
    "paragraph_end_idx = paragraph_start_idx + len(paragraph)\n",
    "output.append(f\"T2\\tParagraph {paragraph_start_idx} {paragraph_end_idx}\\t{paragraph}\")\n",
    "\n",
    "# Perform NER on the paragraph\n",
    "doc = nlp(paragraph)\n",
    "\n",
    "# Start entity IDs from T3\n",
    "entity_id = 3\n",
    "for ent in doc.ents:\n",
    "    entity_type = ent.label_\n",
    "    start = ent.start_char + paragraph_start_idx\n",
    "    end = ent.end_char + paragraph_start_idx\n",
    "    entity_text = ent.text\n",
    "    output.append(f\"T{entity_id}\\t{entity_type} {start} {end}\\t{entity_text}\")\n",
    "    entity_id += 1\n",
    "\n",
    "# Print the output\n",
    "print('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d483fe1-b4a7-4ae7-a033-069bd8f99bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_best_match(input_text, onto_entities):\n",
    "    onto_match_id, onto_match_name, onto_similarity = check_ontobiotope_match(input_text, onto_entities)\n",
    "    \n",
    "    # Handle the case where similarity is None\n",
    "    if onto_similarity is None:\n",
    "        onto_similarity = 0\n",
    "    \n",
    "    return \"Habitat or Phenotype\", onto_match_name, onto_match_id\n",
    "\n",
    "# Load the pre-trained biomedical Named Entity Recognition (NER) model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"The etiologic and epidemiologic spectrum of bronchiolitis in pediatric practice.\n",
    "To develop a broad understanding of the causes and patterns of occurrence of wheezing associated respiratory infections, we analyzed data from an 11-year study of acute lower respiratory illness in a pediatric practice. Although half of the WARI occurred in children less than 2 years of age, wheezing continued to be observed in 19% of children greater than 9 years of age who had lower respiratory illness. Males experienced LRI 1.25 times more often than did females; the relative risk of males for WARI was 1.35. A nonbacterial pathogen was recovered from 21% of patients with WARI; respiratory syncytial virus, parainfluenza virus types 1 and 3, adenoviruses, and Mycoplasma pneumoniae accounted for 81% of the isolates. Patient age influenced the pattern of recovery of these agents. The most common cause of WARI in children under 5 years of age was RSV whereas Mycoplasma pneumoniae was the most frequent isolate from school age children with wheezing illness. The data expand our understanding of the causes of WARI and are useful to diagnosticians and to researchers interested in the control of lower respiratory disease.\"\"\"\n",
    "\n",
    "# Split the text into title and paragraph\n",
    "title_end_idx = text.find('\\n')\n",
    "title = text[:title_end_idx].strip()\n",
    "paragraph = text[title_end_idx + 1:].strip()\n",
    "\n",
    "# Output in .a1 format\n",
    "output = []\n",
    "\n",
    "# Add the title as T1\n",
    "output.append(f\"T1\\tTitle 0 {title_end_idx}\\t{title}\")\n",
    "\n",
    "# Add the paragraph as T2\n",
    "paragraph_start_idx = title_end_idx + 1\n",
    "paragraph_end_idx = paragraph_start_idx + len(paragraph)\n",
    "output.append(f\"T2\\tParagraph {paragraph_start_idx} {paragraph_end_idx}\\t{paragraph}\")\n",
    "\n",
    "# Perform NER on the paragraph\n",
    "doc = nlp(paragraph)\n",
    "\n",
    "# Start entity IDs from T3\n",
    "entity_id = 3\n",
    "for ent in doc.ents:\n",
    "    entity_text = ent.text\n",
    "    \n",
    "    # Get the best match\n",
    "    entity_type, match_name, match_id = adjusted_best_match(entity_text, onto_entities)\n",
    "    \n",
    "    # If the entity_text is present in onto_entities, update entity_type\n",
    "    if match_id:\n",
    "        # Add the entity to the output\n",
    "        start = ent.start_char + paragraph_start_idx\n",
    "        end = ent.end_char + paragraph_start_idx\n",
    "        output.append(f\"T{entity_id}\\t{entity_type} {start} {end}\\t{entity_text}\")\n",
    "        entity_id += 1\n",
    "\n",
    "# Print the output\n",
    "print('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8142f-60ea-4f96-a941-34ff56170ab2",
   "metadata": {},
   "source": [
    "#### Creating A1 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9fddaa-347b-4d69-a64d-982f3531f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the entity matches from input text files and save them as a1 files\n",
    "def process_text_files(input_dir, output_dir, onto_entities):\n",
    "    # Load the pre-trained biomedical Named Entity Recognition (NER) model\n",
    "    nlp = spacy.load(\"en_core_sci_sm\")\n",
    "    \n",
    "    # Iterate over each file in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file_path = os.path.join(input_dir, filename)\n",
    "            output_file_path = os.path.join(output_dir, filename.replace('.txt', '.a1'))\n",
    "            print(f\"Processing: {filename}\")\n",
    "            \n",
    "            # Read the input text file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Split the text into title and paragraph\n",
    "            title_end_idx = text.find('\\n')\n",
    "            title = text[:title_end_idx].strip()\n",
    "            paragraph = text[title_end_idx + 1:].strip()\n",
    "            \n",
    "            # Output in .a1 format\n",
    "            output = []\n",
    "            \n",
    "            # Add the title as T1\n",
    "            output.append(f\"T1\\tTitle 0 {title_end_idx}\\t{title}\")\n",
    "            \n",
    "            # Add the paragraph as T2\n",
    "            paragraph_start_idx = title_end_idx + 1\n",
    "            paragraph_end_idx = paragraph_start_idx + len(paragraph)\n",
    "            output.append(f\"T2\\tParagraph {paragraph_start_idx} {paragraph_end_idx}\\t{paragraph}\")\n",
    "            \n",
    "            # Perform NER on the paragraph\n",
    "            doc = nlp(paragraph)\n",
    "            \n",
    "            # Start entity IDs from T3\n",
    "            entity_id = 3\n",
    "            for ent in doc.ents:\n",
    "                entity_text = ent.text\n",
    "                \n",
    "                # Get the best match\n",
    "                entity_type, match_name, match_id = adjusted_best_match(entity_text, onto_entities)\n",
    "                \n",
    "                # If the entity_text is present in onto_entities, update entity_type\n",
    "                if match_id:\n",
    "                    # Add the entity to the output\n",
    "                    start = ent.start_char + paragraph_start_idx\n",
    "                    end = ent.end_char + paragraph_start_idx\n",
    "                    output.append(f\"T{entity_id}\\t{entity_type} {start} {end}\\t{entity_text}\")\n",
    "                    entity_id += 1\n",
    "            \n",
    "            # Save the entities in .a1 format\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write('\\n'.join(output))\n",
    "            \n",
    "            print(f\"Entities extracted and saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6df018-2da5-4157-9c11-cf581ab9e8e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "a1_output_directory = r\"C:\\Users\\said_\\OneDrive\\Masaüstü\\github\\Natural Language Processing\\Datasets\\Resources\\Trial - A1\"\n",
    "process_text_files(input_file_directory, a1_output_directory, onto_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e398d-2be8-4f38-a248-273c0c6e455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_text_files(directory):\n",
    "    txt_files = [filename for filename in os.listdir(directory) if filename.endswith(\".a1\")]\n",
    "    return len(txt_files)\n",
    "\n",
    "# Example usage\n",
    "num_text_files = count_text_files(a1_output_directory)\n",
    "print(f\"Number of text files in the directory: {num_text_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded106b-a944-4852-9a5c-f958539ad395",
   "metadata": {},
   "source": [
    "#### 3) Creating A2 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b593d-575f-4a16-94be-5423a3b2620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files(input_dir, output_dir, onto_entities):\n",
    "    # Load the pre-trained biomedical Named Entity Recognition (NER) model\n",
    "    nlp = spacy.load(\"en_core_sci_sm\")\n",
    "    \n",
    "    # Iterate over each file in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file_path = os.path.join(input_dir, filename)\n",
    "            output_file_path = os.path.join(output_dir, filename.replace('.txt', '.a2'))\n",
    "            print(f\"Processing: {filename}\")\n",
    "            \n",
    "            # Read the input text file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Split the text into title and paragraph\n",
    "            title_end_idx = text.find('\\n')\n",
    "            title = text[:title_end_idx].strip()\n",
    "            paragraph = text[title_end_idx + 1:].strip()\n",
    "            \n",
    "            # Perform NER on the paragraph\n",
    "            doc = nlp(paragraph)\n",
    "            \n",
    "            # Start entity IDs from T3\n",
    "            entity_id = 3\n",
    "            # Entities to be included in a1 format\n",
    "            entities_a1 = []\n",
    "            additional_annotations = []\n",
    "\n",
    "            for ent in doc.ents:\n",
    "                entity_text = ent.text\n",
    "                \n",
    "                # Get the best match\n",
    "                entity_type, match_name, match_id = adjusted_best_match(entity_text, onto_entities)\n",
    "                \n",
    "                # If the entity_text is present in onto_entities, update entity_type\n",
    "                if match_id:\n",
    "                    # Add the entity to the output (excluding T1 and T2)\n",
    "                    start = ent.start_char + title_end_idx + 1\n",
    "                    end = ent.end_char + title_end_idx + 1\n",
    "                    entities_a1.append(f\"T{entity_id}\\t{entity_type} {start} {end}\\t{entity_text}\")\n",
    "                    entity_id += 1\n",
    "\n",
    "                    # Add additional annotation\n",
    "                    # Skip the first two entities (T1 and T2)\n",
    "                    if entity_id > 3:\n",
    "                        additional_annotations.append(f\"N{entity_id-3}\\tOntoBiotope Annotation:T{entity_id-1} Referent:{match_id}\")\n",
    "            \n",
    "            # Save the entities in .a1 format\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write('\\n'.join(entities_a1 + additional_annotations))\n",
    "            \n",
    "            print(f\"Entities extracted and saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4f6b8-c78f-4dab-ae61-3cdf22c8757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "a2_output_directory = r\"C:\\Users\\said_\\OneDrive\\Masaüstü\\github\\Natural Language Processing\\Datasets\\Resources\\Trial - A2\"\n",
    "process_text_files(input_file_directory, a2_output_directory, onto_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21c4ae-33fd-4040-97d6-d65805321ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_text_files(directory):\n",
    "    txt_files = [filename for filename in os.listdir(directory) if filename.endswith(\".a2\")]\n",
    "    return len(txt_files)\n",
    "\n",
    "# Example usage\n",
    "num_text_files = count_text_files(a2_output_directory)\n",
    "print(f\"Number of text files in the directory: {num_text_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449109e3-6a02-4d4c-83cf-c195a0cb8689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156acfb3-d34d-4d2f-8313-920879e4f481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c40a009-cecf-4062-a52c-00cd01da33cf",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
